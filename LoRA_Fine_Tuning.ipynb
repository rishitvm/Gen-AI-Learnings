{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1a9e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f637dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a LoRA class\n",
    "\n",
    "class LoRAAdapter(nn.Module):\n",
    "    def __init__(self, hidden_size, r=4, alpha=16):\n",
    "        super().__init__()\n",
    "        self.r = r              # Rank\n",
    "        self.alpha = alpha      # To get the scaling factor\n",
    "        self.scaling = self.alpha / self.r\n",
    "        self.A = nn.Parameter(torch.randn(r, hidden_size) * 0.01)  # Downsampler\n",
    "        self.B = nn.Parameter(torch.randn(hidden_size, r) * 0.01)  # Upsampler\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x @ self.A.T @ self.B.T) * self.scaling            # Forward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ccea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding LoRA to model at each layer\n",
    "\n",
    "def add_lora_to_gpt2(model, r=4, alpha=16):\n",
    "    for block in model.transformer.h:                       # Accessing each decoder block\n",
    "        hidden_size = block.attn.embed_dim                 \n",
    "        lora = LoRAAdapter(hidden_size, r, alpha)           # Call the LoRA Class with the embedding size as it is the same for Q,K,V\n",
    "\n",
    "        def hook(module, input, output, lora = lora):       # A hook to be defined so that it resides in cache and no need to define it always\n",
    "            hidden = input[0]                               # Pass the input tensor\n",
    "            q, k, v = output.split(hidden_size, dim = 2)    # Extract the Q,K,V weights from the attention block\n",
    "            q = q + lora(hidden)                            # Add the Q and LoRA result\n",
    "            v = v + lora(hidden)                            # Add the V and LoRA result\n",
    "            return torch.cat([q, k, v], dim = 2)            # Concatenate the final result\n",
    "\n",
    "        block.attn.c_attn.register_forward_hook(hook)       # Register the hook\n",
    "        block.attn.lora = lora                              # Save the LoRA weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440849a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Dataset and DataLoader for batching of inputs\n",
    "\n",
    "class JokeDataset(Dataset):\n",
    "    def __init__(self, tokenizer, jokes, block_size = 64):      # We pass a tokeniser to generate tokens for each sentence in the dataset\n",
    "        self.inputs = []\n",
    "        for joke in jokes:\n",
    "            enc = tokenizer(joke, truncation = True, max_length = block_size, return_tensors='pt')\n",
    "            self.inputs.append(enc.input_ids.squeeze(0))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "         return self.inputs[idx], self.inputs[idx]\n",
    "\n",
    "def collate_fn(batch):                                          # Makes use of Padding with a particular token for variable length sequences\n",
    "    inputs, targets = zip(*batch)\n",
    "    inputs = nn.utils.rnn.pad_sequence(inputs, batch_first = True, padding_value = tokenizer.pad_token_id)\n",
    "    return inputs, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e14ff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a global token (here EOS) to pad the variable length sequences\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "add_lora_to_gpt2(model, r=4, alpha=16)\n",
    "\n",
    "for name, param in model.named_parameters():       # Freeze all the trainable parameters\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for block in model.transformer.h:\n",
    "    for param in block.attn.lora.parameters():     # Only unfreeze the LoRA parameters present in attention layer\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b298790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small dataset of sentences with jokes\n",
    "\n",
    "jokes = [\n",
    "    \"Why did the chicken join a band? Because it had the drumsticks!\",\n",
    "    \"I told my computer I needed a break, and it said 'No problem, I'll go to sleep.'\",\n",
    "    \"Why don't scientists trust atoms? Because they make up everything!\",\n",
    "    \"Parallel lines have so much in common. It’s a shame they’ll never meet.\"\n",
    "]\n",
    "\n",
    "dataset = JokeDataset(tokenizer, jokes)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47626c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 5.4883\n",
      "Epoch 0, Loss: 5.9421\n",
      "Epoch 1, Loss: 5.4231\n",
      "Epoch 1, Loss: 6.0338\n",
      "Epoch 2, Loss: 6.0228\n",
      "Epoch 2, Loss: 5.5075\n",
      "Epoch 3, Loss: 5.8277\n",
      "Epoch 3, Loss: 5.5118\n",
      "Epoch 4, Loss: 5.3165\n",
      "Epoch 4, Loss: 3.6309\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Apply an optimiser but only for parameters which are made trainable (i.e. here only for LoRA parameters)\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "# Train the fine-tuned model for 5 epochs\n",
    "for epoch in range(5):\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs, labels = targets)\n",
    "        loss = outputs.loss                                        # Computes the loss (Cross-Entropy)\n",
    "        loss.backward()                                            # Backpropagation\n",
    "        optimizer.step()                                   \n",
    "        optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "lora_state = {\n",
    "    f\"block_{i}_lora\": block.attn.lora.state_dict()\n",
    "    for i, block in enumerate(model.transformer.h)\n",
    "}\n",
    "torch.save(lora_state, \"lora_adapters.pt\")                         # Save the LoRA Parameters for future use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7026f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did Trump’s candidacy work? Did it work for the voters who cared about him? Did he lose voters or did he just lose people?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I have an\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "\n",
    "model.eval()\n",
    "input_ids = tokenizer(\"Why did\", return_tensors=\"pt\").input_ids.to(device)\n",
    "output = model.generate(input_ids, max_length=40, do_sample=True)\n",
    "print(tokenizer.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5874b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Original GPT-2 Output:\n",
      "Why did the chicken cross the road?‬This is an interesting question: Why is there a way that a chicken can cross a road even if the chicken didn‬then, so could an animal not cross a road? Does the dog walk\n"
     ]
    }
   ],
   "source": [
    "# Get the output using the base model\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "prompt = \"Why did the chicken cross the road?\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "original_model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\").to(device)\n",
    "original_model.eval()\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "original_output = original_model.generate(input_ids, max_length=50, do_sample=True)\n",
    "original_text = tokenizer.decode(original_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n Original GPT-2 Output:\")\n",
    "print(original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a7e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M Rishit Varma\\AppData\\Local\\Temp\\ipykernel_5596\\2338108257.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lora_state = torch.load(\"lora_adapters.pt\", map_location = device)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LoRA Fine-Tuned Output:\n",
      "Why did the chicken cross the road? And did the chickens even cross the road? And did the chicken fly? Did the chicken cross the road? Did the chickens even cross the road? And did the chicken fly? Did the chicken fly?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the LoRA weights and use the same input question on this fine-tuned model\n",
    "\n",
    "lora_model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "add_lora_to_gpt2(lora_model, r=4, alpha=16)\n",
    "\n",
    "for name, param in lora_model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "lora_state = torch.load(\"lora_adapters.pt\", map_location = device)\n",
    "for i, block in enumerate(lora_model.transformer.h):\n",
    "    block.attn.lora.load_state_dict(lora_state[f\"block_{i}_lora\"])\n",
    "\n",
    "lora_model.to(device)\n",
    "lora_model.eval()\n",
    "\n",
    "lora_output = lora_model.generate(input_ids, max_length=50, do_sample=True)\n",
    "lora_text = tokenizer.decode(lora_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n LoRA Fine-Tuned Output:\")\n",
    "print(lora_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
