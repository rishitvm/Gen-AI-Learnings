{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd1a9e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f637dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAAdapter(nn.Module):\n",
    "    def __init__(self, hidden_size, r=4, alpha=16):\n",
    "        super().__init__()\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = self.alpha / self.r\n",
    "        self.A = nn.Parameter(torch.randn(r, hidden_size) * 0.01)\n",
    "        self.B = nn.Parameter(torch.randn(hidden_size, r) * 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x @ self.A.T @ self.B.T) * self.scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6ccea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lora_to_gpt2(model, r=4, alpha=16):\n",
    "    for block in model.transformer.h:\n",
    "        hidden_size = block.attn.embed_dim\n",
    "        lora = LoRAAdapter(hidden_size, r, alpha)\n",
    "\n",
    "        def hook(module, input, output, lora = lora):\n",
    "            hidden = input[0]\n",
    "            q, k, v = output.split(hidden_size, dim = 2)\n",
    "            q = q + lora(hidden)\n",
    "            v = v + lora(hidden)\n",
    "            return torch.cat([q, k, v], dim = 2)\n",
    "\n",
    "        block.attn.c_attn.register_forward_hook(hook)\n",
    "        block.attn.lora = lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "440849a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JokeDataset(Dataset):\n",
    "    def __init__(self, tokenizer, jokes, block_size = 64):\n",
    "        self.inputs = []\n",
    "        for joke in jokes:\n",
    "            enc = tokenizer(joke, truncation = True, max_length = block_size, return_tensors='pt')\n",
    "            self.inputs.append(enc.input_ids.squeeze(0))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "         return self.inputs[idx], self.inputs[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    inputs = nn.utils.rnn.pad_sequence(inputs, batch_first = True, padding_value = tokenizer.pad_token_id)\n",
    "    return inputs, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e14ff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "add_lora_to_gpt2(model, r=4, alpha=16)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for block in model.transformer.h:\n",
    "    for param in block.attn.lora.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b298790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes = [\n",
    "    \"Why did the chicken join a band? Because it had the drumsticks!\",\n",
    "    \"I told my computer I needed a break, and it said 'No problem, I'll go to sleep.'\",\n",
    "    \"Why don't scientists trust atoms? Because they make up everything!\",\n",
    "    \"Parallel lines have so much in common. It’s a shame they’ll never meet.\"\n",
    "]\n",
    "\n",
    "dataset = JokeDataset(tokenizer, jokes)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47626c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 5.4883\n",
      "Epoch 0, Loss: 5.9421\n",
      "Epoch 1, Loss: 5.4231\n",
      "Epoch 1, Loss: 6.0338\n",
      "Epoch 2, Loss: 6.0228\n",
      "Epoch 2, Loss: 5.5075\n",
      "Epoch 3, Loss: 5.8277\n",
      "Epoch 3, Loss: 5.5118\n",
      "Epoch 4, Loss: 5.3165\n",
      "Epoch 4, Loss: 3.6309\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "for epoch in range(5):\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs, labels = targets)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "lora_state = {\n",
    "    f\"block_{i}_lora\": block.attn.lora.state_dict()\n",
    "    for i, block in enumerate(model.transformer.h)\n",
    "}\n",
    "torch.save(lora_state, \"lora_adapters.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e7026f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did Trump’s candidacy work? Did it work for the voters who cared about him? Did he lose voters or did he just lose people?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I have an\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "input_ids = tokenizer(\"Why did\", return_tensors=\"pt\").input_ids.to(device)\n",
    "output = model.generate(input_ids, max_length=40, do_sample=True)\n",
    "print(tokenizer.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5874b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Original GPT-2 Output:\n",
      "Why did the chicken cross the road?‬This is an interesting question: Why is there a way that a chicken can cross a road even if the chicken didn‬then, so could an animal not cross a road? Does the dog walk\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "prompt = \"Why did the chicken cross the road?\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "original_model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\").to(device)\n",
    "original_model.eval()\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "original_output = original_model.generate(input_ids, max_length=50, do_sample=True)\n",
    "original_text = tokenizer.decode(original_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n Original GPT-2 Output:\")\n",
    "print(original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da4a7e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M Rishit Varma\\AppData\\Local\\Temp\\ipykernel_5596\\2338108257.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lora_state = torch.load(\"lora_adapters.pt\", map_location = device)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LoRA Fine-Tuned Output:\n",
      "Why did the chicken cross the road? And did the chickens even cross the road? And did the chicken fly? Did the chicken cross the road? Did the chickens even cross the road? And did the chicken fly? Did the chicken fly?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lora_model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "add_lora_to_gpt2(lora_model, r=4, alpha=16)\n",
    "\n",
    "for name, param in lora_model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "lora_state = torch.load(\"lora_adapters.pt\", map_location = device)\n",
    "for i, block in enumerate(lora_model.transformer.h):\n",
    "    block.attn.lora.load_state_dict(lora_state[f\"block_{i}_lora\"])\n",
    "\n",
    "lora_model.to(device)\n",
    "lora_model.eval()\n",
    "\n",
    "lora_output = lora_model.generate(input_ids, max_length=50, do_sample=True)\n",
    "lora_text = tokenizer.decode(lora_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n LoRA Fine-Tuned Output:\")\n",
    "print(lora_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
