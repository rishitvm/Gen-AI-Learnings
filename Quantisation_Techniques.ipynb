{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23a0f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2b9ae9",
   "metadata": {},
   "source": [
    "### QLoRA (NF 4 + Double Quantisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1030e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NF4 requires a Look Up table to substitute the values with nearest value\n",
    "\n",
    "def nf4_lookuptable(weight):\n",
    "    nf4_table = [\n",
    "    -1.0,\n",
    "    -0.6961928,\n",
    "    -0.52507305,\n",
    "    -0.3949175,\n",
    "    -0.28444138,\n",
    "    -0.18477343,\n",
    "    -0.09105027,\n",
    "    -0.0,\n",
    "     0.0,\n",
    "     0.0795803,\n",
    "     0.1609302,\n",
    "     0.24611232,\n",
    "     0.33791524,\n",
    "     0.44070983,\n",
    "     0.562617,\n",
    "     1.0 ]\n",
    "    \n",
    "    # Substitues the nearest value and for easier flow we return the index of the closest weight from look up table\n",
    "\n",
    "    min_value = float(\"inf\")\n",
    "    index = -1\n",
    "    for i in range(16):\n",
    "        value = abs(nf4_table[i] - weight)     # We take the absoulte difference of 2 weights\n",
    "        if value <= min_value:\n",
    "            index = i\n",
    "            min_value = value\n",
    "    return index                               # Returns the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a69b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the quantized matrix given a pre-trained / normal weight matrix\n",
    "\n",
    "def find_nf4(weight_matrix):\n",
    "    quantised_matrix = []\n",
    "    for i in range(len(weight_matrix)):\n",
    "        temp = []\n",
    "        for j in range(len(weight_matrix[0])):\n",
    "            inter = nf4_lookuptable(weight_matrix[i][j])    # Send each weight to the lookup definition to get the closest weight\n",
    "            temp.append(inter)  \n",
    "        quantised_matrix.append(temp)\n",
    "    return quantised_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc0cdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3733, -0.5848, -0.9635,  0.6661, -0.9457],\n",
       "        [-0.1754,  0.7782,  0.6323,  0.5214, -0.9463],\n",
       "        [ 0.9396,  0.1672,  0.2616, -0.2405,  0.8747],\n",
       "        [-0.2828,  0.1751, -0.0074, -0.8564,  0.5614],\n",
       "        [-0.7298,  0.6567,  0.6597, -0.5610, -0.3041]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a sample weight matrix filled with random numbers for easier inference\n",
    "\n",
    "sample_weight_matrix = torch.rand(size = (5,5), dtype = torch.float32) * 2 - 1\n",
    "sample_weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d0fbc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nf4_table = [\n",
    "    -1.0,\n",
    "    -0.6961928,\n",
    "    -0.52507305,\n",
    "    -0.3949175,\n",
    "    -0.28444138,\n",
    "    -0.18477343,\n",
    "    -0.09105027,\n",
    "    -0.0,\n",
    "     0.0,\n",
    "     0.0795803,\n",
    "     0.1609302,\n",
    "     0.24611232,\n",
    "     0.33791524,\n",
    "     0.44070983,\n",
    "     0.562617,\n",
    "     1.0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28663b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12,  2,  0, 14,  0],\n",
       "        [ 5, 14, 14, 14,  0],\n",
       "        [15, 10, 11,  4, 15],\n",
       "        [ 4, 10,  8,  0, 14],\n",
       "        [ 1, 14, 14,  2,  4]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the quantised matrix after applying NF-4 quantisation (each number lookup index is represented in 4 bits 0 - 15 digits)\n",
    "# These will be stored in Memory and during fetching we get the weight at respective index using this matrix from the look up table\n",
    "\n",
    "quantised_weight_matrix = torch.tensor(find_nf4(sample_weight_matrix))\n",
    "quantised_weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb1301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3379, -0.5251, -1.0000,  0.5626, -1.0000],\n",
       "        [-0.1848,  0.5626,  0.5626,  0.5626, -1.0000],\n",
       "        [ 1.0000,  0.1609,  0.2461, -0.2844,  1.0000],\n",
       "        [-0.2844,  0.1609,  0.0000, -1.0000,  0.5626],\n",
       "        [-0.6962,  0.5626,  0.5626, -0.5251, -0.2844]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the quantized matrix whoch consists of the nearest look up weights\n",
    "\n",
    "quantized_weights = []\n",
    "\n",
    "for x in quantised_weight_matrix:\n",
    "    temp = []\n",
    "    for y in x:\n",
    "        temp.append(nf4_table[y])\n",
    "    quantized_weights.append(temp)\n",
    "\n",
    "quantized_weights = torch.tensor(quantized_weights)\n",
    "quantized_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879ed607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0353, -0.0597,  0.0365,  0.1035,  0.0543],\n",
       "        [ 0.0093,  0.2156,  0.0697, -0.0412,  0.0537],\n",
       "        [-0.0604,  0.0063,  0.0155,  0.0440, -0.1253],\n",
       "        [ 0.0016,  0.0142, -0.0074,  0.1436, -0.0012],\n",
       "        [-0.0336,  0.0940,  0.0971, -0.0360, -0.0197]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss in weights (Information Lost in NF4)\n",
    "\n",
    "sample_weight_matrix - quantized_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed46bc2",
   "metadata": {},
   "source": [
    "### GPTQ (Generative Pre-Trained Quantisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b2d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install auto-gptq transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae298cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using inbuilt GPTQ library for Inference\n",
    "# GPTQ tries to minimise the difference in the layer outputs row wise \n",
    "# It finds for parameters like scaling factor and offset value which gives the least loss in output\n",
    "# It applies those parameters for future inputs or data to reduce the same loss \n",
    "\n",
    "import torch\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"facebook/opt-125m\"    # Model Name\n",
    "\n",
    "# Tokenizer and AutoModel respectively\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Configuring the quantisation metrics\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,                     # Store in 4 bits\n",
    "    group_size=128,\n",
    "    desc_act=False,\n",
    ")\n",
    "\n",
    "# Defining the quantisation model using the base model and defined quantisation paramerters\n",
    "quant_model = AutoGPTQForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantize_config = quantize_config,\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "\n",
    "# A smaple input to be sent for GPTQ to get the optimal parameters for quantisation\n",
    "example_inputs = tokenizer(\"Hello, this is a test input for GPTQ.\", return_tensors=\"pt\")\n",
    "\n",
    "# Quantise the model\n",
    "quant_model.quantize([example_inputs])\n",
    "\n",
    "orig_weight = model.model.decoder.layers[0].self_attn.q_proj.weight             # Original Unquantised weights at Layer 0 attention q matrix\n",
    "quant_layer = quant_model.model.model.decoder.layers[0].self_attn.q_proj        # Quantised weights at Layer 0 attention q matrix\n",
    "\n",
    "print(\"Original Weight (first 5x5 block):\")\n",
    "print(orig_weight[:5, :5].to(torch.float32))\n",
    "print(\"\\nQuantized Weight (first 5x5 block):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2adab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print small part of the quantised matrix from that layer\n",
    "\n",
    "print(quant_layer.qweight[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab7ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically torch uses int32 and int 64 to store the weights but not int4 or int8.\n",
    "# So torch stores the quantised values each of 4 bits in torch.int32 format\n",
    "# So we have to shift respectively to get the individual weights\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "packed_int32_tensor = quant_layer.qweight.cpu().numpy()\n",
    "orig_shape = orig_weight.shape\n",
    "num_int32 = packed_int32_tensor.size\n",
    "num_4bit = num_int32 * 8\n",
    "packed_int32_flat = packed_int32_tensor.flatten()\n",
    "unpacked_4bit = np.zeros(num_4bit, dtype=np.int8)\n",
    "\n",
    "for i, val in enumerate(packed_int32_flat):\n",
    "    for j in range(8):\n",
    "        nibble = (val >> (4 * j)) & 0xF\n",
    "        if nibble >= 8:\n",
    "            nibble -= 16\n",
    "        unpacked_4bit[i*8 + j] = nibble\n",
    "\n",
    "rows, cols = orig_shape\n",
    "unpacked_weight_matrix = unpacked_4bit.reshape(rows, cols)\n",
    "print(\"Unpacked 4-bit quantized weights (first 5x5 block):\")\n",
    "print(unpacked_weight_matrix[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1a32dc",
   "metadata": {},
   "source": [
    "### AWQ (Activation aware Weight Quantisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f92822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as GPTQ but this AWQ tries to get to know which values are sensitive to activation\n",
    "# i.e on which input, which output affects more, so those type of weights are held differently and others differently\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"OriDragon2000/Qwen1.5-1.8B-awq-w4-g128\"               # QWEN model (AWQ model - already quantised)\n",
    "\n",
    "# AWQ requires some external hardware to use it and due to unavailablilty already quantised AWQ model is taken for inference\n",
    "\n",
    "# Tokenizer and mdoel for QWEN\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "# Sample Input text\n",
    "input_text = \"What is the capital of France?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate output for given prompt\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the quantised weights\n",
    "print((model.model.layers[0].mlp.down_proj.qweight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfd9888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to GPTQ, torch uses torch.int32 to store the values which are quantised so we need to split manually to see them\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "quant_layer = model.model.layers[0].mlp.down_proj\n",
    "\n",
    "packed_int32_tensor = quant_layer.qweight.cpu().numpy()\n",
    "orig_shape = quant_layer.out_features, quant_layer.in_features\n",
    "num_int32 = packed_int32_tensor.size\n",
    "num_4bit = num_int32 * 8\n",
    "packed_int32_flat = packed_int32_tensor.flatten()\n",
    "unpacked_4bit = np.zeros(num_4bit, dtype=np.int8)\n",
    "\n",
    "for i, val in enumerate(packed_int32_flat):\n",
    "    for j in range(8):\n",
    "        nibble = (val >> (4 * j)) & 0xF\n",
    "        if nibble >= 8:\n",
    "            nibble -= 16\n",
    "        unpacked_4bit[i * 8 + j] = nibble\n",
    "\n",
    "unpacked_weight_matrix = unpacked_4bit.reshape(orig_shape)\n",
    "\n",
    "print(\"Unpacked 4-bit quantized weights (5x5 block):\")\n",
    "print(unpacked_weight_matrix[:5, :5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
