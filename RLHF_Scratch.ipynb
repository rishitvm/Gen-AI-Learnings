{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! pip install transformers accelerate peft datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"OpenAssistant/oasst1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset[\"train\"]\n",
    "\n",
    "# Preprocessing the dataset to get the proper and valid responses\n",
    "\n",
    "sft_data = []\n",
    "counter = 0\n",
    "\n",
    "for row in dataset:\n",
    "    if row[\"role\"] == \"assistant\" and row.get(\"parent_id\") is not None:\n",
    "        parent = next((item for item in dataset if item[\"message_id\"] == row[\"parent_id\"]), None)\n",
    "        if parent and parent[\"role\"] == \"prompter\":\n",
    "            counter += 1\n",
    "            sft_data.append({\"prompt\": parent[\"text\"],\"response\": row[\"text\"]})\n",
    "            if counter == 500:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(sft_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Model and Tokenizer initialisation\n",
    "\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize definition to generate tokens for prompt + response text\n",
    "\n",
    "def tokenize(example):\n",
    "    input_text = example[\"prompt\"] + \"\\n\\n\" + example[\"response\"]\n",
    "    return tokenizer(input_text, truncation = True, max_length = 256, padding = \"max_length\")\n",
    "\n",
    "# Map the function with original dataset\n",
    "\n",
    "tokenized_data = list(map(tokenize, sft_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenized_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "dataset = Dataset.from_list(tokenized_data)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# LoRA fine tuning to get the SFT Model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 8,\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.1,\n",
    "    target_modules = [\"c_attn\"],\n",
    "    bias = \"none\",\n",
    "    task_type = TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# SFT Model\n",
    "model = get_peft_model(model,lora_config)\n",
    "\n",
    "# Freezinf the Pre Trained Weights other than LoRA Params\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Batching data\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)\n",
    "\n",
    "# Training Args required for Trainer Class\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-distilgpt2\",\n",
    "    per_device_train_batch_size = 4,\n",
    "    num_train_epochs = 3,\n",
    "    eval_strategy = \"no\",\n",
    "    fp16 = torch.cuda.is_available(),\n",
    "    save_total_limit = 2,\n",
    "    remove_unused_columns = False,\n",
    "    report_to = \"none\"\n",
    ")\n",
    "\n",
    "# Trainer class invoke\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    "    data_collator = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate datasets peft trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load a reward model \n",
    "\n",
    "model_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Inference of that model\n",
    "question = \"Explain nuclear fusion like I am five.\"\n",
    "answer = \"Nuclear fusion is the process by which two or more protons and neutrons combine to form a single nucleus.\"\n",
    "\n",
    "inputs = tokenizer(question, answer, return_tensors='pt')\n",
    "\n",
    "outputs = model(**inputs)\n",
    "score = outputs.logits[0].cpu().detach().numpy()\n",
    "\n",
    "# Reward Score for the prompt\n",
    "print(f\"Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "from trl import PPOTrainer, PPOConfig\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "base_model_name = \"distilgpt2\"             # Base Model\n",
    "sft_model_path = \"/kaggle/working/lora-distilgpt2/checkpoint-189\"      # Supervised Fine Tuned (SFT) Model path\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)             # Tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token                              # PAD Token\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name)         \n",
    "model = PeftModel.from_pretrained(model, sft_model_path)               # Supervised Fine Tuned (SFT) Model\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(\"OpenAssistant/reward-model-deberta-v3-large-v2\")       # Reward Tokenizer\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\"OpenAssistant/reward-model-deberta-v3-large-v2\")   # Reward Model\n",
    "reward_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# PPO Configuration parameters\n",
    " \n",
    "ppo_config = PPOConfig(\n",
    "    output_dir=\"./ppo_output\",\n",
    "    learning_rate=1e-5,\n",
    "    batch_size=1,\n",
    "    mini_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    ")\n",
    "\n",
    "# Testing Prompts\n",
    "\n",
    "prompts = [\n",
    "    \"What is reinforcement learning?\",\n",
    "    \"How does a black hole form?\",\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"Why is the sky blue?\",\n",
    "    \"Tell me a fun fact about animals.\"\n",
    "]\n",
    "\n",
    "# Keep all the prompts in a HuggingFace Format\n",
    "dataset = Dataset.from_dict({\"prompt\": prompts})\n",
    "\n",
    "# Reward Score generator definiton given a prompt\n",
    "def get_reward(prompt, response):\n",
    "    with torch.no_grad():\n",
    "        inputs = reward_tokenizer(prompt, response, return_tensors=\"pt\", padding=True, truncation=True).to(reward_model.device)\n",
    "        reward = reward_model(**inputs).logits[0].item()\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "from trl import PPOTrainer\n",
    "\n",
    "# TRL - Transformer Reinforcement Learning\n",
    "\n",
    "print(inspect.signature(PPOTrainer.__init__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,             # PPOConfig instance\n",
    "    processing_class=tokenizer,    # tokenizer or processor\n",
    "    policy=model,                  # your LoRA fine-tuned model (policy network)\n",
    "    ref_policy=AutoModelForCausalLM.from_pretrained(base_model_name),  # base model without LoRA\n",
    "    reward_model=reward_model,    # your reward model\n",
    "    train_dataset=dataset,        # Dataset of prompts\n",
    "    value_model=model       # value function model (usually a causal LM too)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class LoggingCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        print(f\"Step {state.global_step}: {logs}\")\n",
    "\n",
    "# PPO Trainer class and their parameters\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    processing_class=tokenizer,\n",
    "    policy=model,\n",
    "    ref_policy=AutoModelForCausalLM.from_pretrained(base_model_name),\n",
    "    reward_model=reward_model,\n",
    "    train_dataset=dataset,\n",
    "    value_model=model,\n",
    "    callbacks=[LoggingCallback()]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "ppo_trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "ppo_trainer.save_pretrained(\"./ppo_output\")\n",
    "\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Load the PPO Optimized Model\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "ppo_model = PeftModel.from_pretrained(base_model, \"./ppo_output\")\n",
    "ppo_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Test Prompts\n",
    "\n",
    "test_prompts = [\n",
    "    \"Explain reinforcement learning simply.\",\n",
    "    \"What is a black hole?\",\n",
    "    \"Tell me about quantum physics.\"\n",
    "]\n",
    "\n",
    "# Perform Inference\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(ppo_model.device)\n",
    "    generated_ids = ppo_model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        max_length=100,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.8,\n",
    "    )\n",
    "    response = tokenizer.decode(generated_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    print(f\"\\nPrompt: {prompt}\\nResponse: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Print reward for each prompt\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    response = ...\n",
    "    reward = get_reward(prompt, response)\n",
    "    print(f\"Reward: {reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from trl import DPOTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./dpo-distilgpt2\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"no\",\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    args=training_args,\n",
    "    beta=0.1,\n",
    "    train_dataset=dpo_dataset,  # must have 'prompt', 'chosen', 'rejected'\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./dpo-aligned-distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./dpo-aligned-distilgpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate a response\n",
    "prompt = \"Explain reinforcement learning in simple terms.\"\n",
    "response = generator(prompt, max_length=100, do_sample=True, top_k=50, top_p=0.95, temperature=0.8)\n",
    "print(response[0][\"generated_text\"])\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
